{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New behaviors: \n",
      "    user_id                                     candidate_news  \\\n",
      "0    22779  [9774516, 9771051, 9770028, 9775402, 9774461, ...   \n",
      "1   150224  [9778669, 9778736, 9778623, 9089120, 9778661, ...   \n",
      "2   160892  [9778369, 9777856, 9778500, 9778021, 9778627, ...   \n",
      "3  1001055  [9776715, 9776406, 9776566, 9776071, 9776808, ...   \n",
      "4  1001055  [9775202, 9776855, 9776688, 9771995, 9776583, ...   \n",
      "\n",
      "  article_ids_clicked                                        clicked_idx  \n",
      "0           [9759966]                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
      "1           [9778661]  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2           [9777856]                  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "3           [9776566]                        [0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
      "4           [9776553]  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "Loading GloVe vectors from saved file: ../Data/glove_vectors.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5105984000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mData_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFinal_Model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oriol\\OneDrive\\Documentos\\dtu\\DL\\DL-project\\Data_loader.py:203\u001b[0m\n\u001b[0;32m    200\u001b[0m     column \u001b[38;5;241m=\u001b[39m pad_sequence(column, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n\u001b[1;32m--> 203\u001b[0m browsed_news_train \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_pad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbrowsed_news\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m candidate_news_train \u001b[38;5;241m=\u001b[39m tensor_pad(input_data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate_news\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    205\u001b[0m clicked_news_train \u001b[38;5;241m=\u001b[39m tensor_pad(input_data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclicked_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\oriol\\OneDrive\\Documentos\\dtu\\DL\\DL-project\\Data_loader.py:200\u001b[0m, in \u001b[0;36mtensor_pad\u001b[1;34m(column)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_pad\u001b[39m(column):\n\u001b[0;32m    199\u001b[0m     column \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(sublist) \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m--> 200\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[1;32mc:\\Users\\oriol\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:478\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[0;32m    474\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5105984000 bytes."
     ]
    }
   ],
   "source": [
    "from Data_loader import *\n",
    "from Final_Model import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nbrowsed news: \", browsed_news_train.shape\n",
    "      , \"\\ncandidate news: \", candidate_news_train.shape\n",
    "      , \"\\nclicked news: \", clicked_news_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------NEWS ENCODER------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glove_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m### Test news encoder\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ------NEWS ENCODER------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m word_embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mglove_vectors\u001b[49m  \u001b[38;5;66;03m# Assume glove_vectors are loaded and of correct size\u001b[39;00m\n\u001b[0;32m      7\u001b[0m attention_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "### Test data loader (do not run if trying to test the training loop)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "### Test news encoder\n",
    "print(\"\\n ------NEWS ENCODER------\")\n",
    "\n",
    "word_embedding_matrix = glove_vectors  # Assume glove_vectors are loaded and of correct size\n",
    "attention_dim = 200\n",
    "# Instantiate the model\n",
    "news_encoder = NewsEncoder(embed_size=300, heads=15, word_embedding_matrix=word_embedding_matrix, attention_dim=attention_dim)\n",
    "\n",
    "# Random input\n",
    "\n",
    "x = browsed_news_train[:batch_size, 1, :] #[Batch size, 1 news, 26 words]\n",
    "\n",
    "output = news_encoder(x)\n",
    "\n",
    "print(\"input shape:\", x.shape)\n",
    "print(\"output shape:\", output.shape) # News encoder works fine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Test user encoder\n",
    "print('\\n ------USER ENCODER------')\n",
    "\n",
    "user_encoder = UserEncoder(embed_size=300, heads=15, attention_dim=200)\n",
    "\n",
    "x = browsed_news_train[:batch_size, :, :] #[Batch size, all news, 26 words]\n",
    "\n",
    "e = [news_encoder(news) for news in x] # Apply the news encoder to each news article\n",
    "e = torch.stack(e, dim=0)\n",
    "\n",
    "\n",
    "output = user_encoder(e)\n",
    "\n",
    "print(\"input shape:\", e.shape)\n",
    "print(\"output shape:\", output.shape) # User encoder works fine\n",
    "\n",
    "### Test full model\n",
    "print('\\n -----COMPLETE MODEL------') \n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "model_final = NRMS(embed_size=300, heads=15, word_embedding_matrix=glove_vectors, attention_dim=200)\n",
    "\n",
    "browsed_news_batch = browsed_news_train[:batch_size, :, :] #[Batch size, all news, 26 words]\n",
    "candidate_news_batch = candidate_news_train[:batch_size, :, :] #[Batch size, all news, 26 words]\n",
    "clicked_news_batch = clicked_news_train[:batch_size, :] #[Batch size, all news]\n",
    "\n",
    "# Forward pass for the entire batch\n",
    "click = model_final(browsed_news_batch, candidate_news_batch)\n",
    "\n",
    "print(f\"\\nInput shape: Browsed = {browsed_news_batch.shape}, Candidate = {candidate_news_batch.shape}\")\n",
    "print(f\"Output shape: Click = {click.shape}\") # Full model works fine\n",
    "\n",
    "print(\"\\nSum of probabilities:\", torch.sum(click, dim=1)) # Probabilities sum to 1\n",
    "print(f\"\\nClicked indices: True = {torch.argmax(click, dim=1)}, Predicted = {torch.argmax(clicked_news_batch, dim=1)}\") # Predicted indices match true indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "model_final = NRMS(embed_size=300, heads=15, word_embedding_matrix=glove_vectors, attention_dim=200)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model_final.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "\n",
    "    for i in range(0, len(browsed_news_train), batch_size):\n",
    "        browsed_news_batch = browsed_news_train[i:i+batch_size, :, :]\n",
    "        candidate_news_batch = candidate_news_train[i:i+batch_size, :, :]\n",
    "        clicked_news_batch = clicked_news_train[i:i+batch_size, :]\n",
    "\n",
    "        # Forward pass\n",
    "        click = model_final(browsed_news_batch, candidate_news_batch)\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(click, torch.argmax(clicked_news_batch, dim=1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(browsed_news_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
